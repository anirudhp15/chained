import { NextRequest, NextResponse } from "next/server";
import { ConvexHttpClient } from "convex/browser";
import { api } from "../../../convex/_generated/api";
import type { Id } from "../../../convex/_generated/dataModel";
import { auth } from "@clerk/nextjs/server";
import { streamLLM } from "../../../lib/llm-stream";

import {
  parseSupervisorPrompt,
  buildSupervisorPrompt,
  buildAgentTaskPrompt,
  validateAgentMentions,
  type MentionTask,
} from "../../../lib/supervisor-parser";

const convex = new ConvexHttpClient(process.env.NEXT_PUBLIC_CONVEX_URL!);

export async function POST(request: NextRequest) {
  try {
    const requestBody = await request.json();
    const { sessionId } = requestBody;

    // Handle both legacy string format and new structured format
    let userInput: string;
    let references: any[] = [];
    let fullContextForAI: string;

    if (typeof requestBody.userInput === "string") {
      // Legacy format - just a string
      userInput = requestBody.userInput;
      fullContextForAI = userInput;
    } else if (requestBody.userInput) {
      // New structured format - parse JSON
      try {
        const parsed = JSON.parse(requestBody.userInput);
        userInput = parsed.userInput;
        references = parsed.references || [];
        fullContextForAI = parsed.fullContext || parsed.userInput;
      } catch (e) {
        // Fallback to treating as string if JSON parsing fails
        userInput = requestBody.userInput;
        fullContextForAI = userInput;
      }
    } else {
      // Direct properties (alternative structured format)
      userInput = requestBody.userInput || "";
      references = requestBody.references || [];
      fullContextForAI = requestBody.fullContext || userInput;
    }

    if (!sessionId || !userInput) {
      return NextResponse.json(
        { error: "Missing sessionId or userInput" },
        { status: 400 }
      );
    }

    // Authenticate user
    const authResult = await auth();
    if (!authResult.userId) {
      return NextResponse.json({ error: "Unauthorized" }, { status: 401 });
    }

    // Set auth token for Convex
    const token = await authResult.getToken({ template: "convex" });
    if (!token) {
      return NextResponse.json(
        { error: "Failed to get auth token" },
        { status: 401 }
      );
    }
    convex.setAuth(token);

    // Fetch chain context (limit to last 5 agents for MVP)
    const agentSteps = await convex.query(api.queries.getAgentSteps, {
      sessionId: sessionId as Id<"chatSessions">,
    });

    if (!agentSteps || agentSteps.length === 0) {
      return NextResponse.json(
        { error: "No agent steps found for this session" },
        { status: 404 }
      );
    }

    // Get supervisor history for context
    const supervisorTurns = await convex.query(api.queries.getSupervisorTurns, {
      sessionId: sessionId as Id<"chatSessions">,
    });

    // Parse user input for @mentions (use fullContext for AI processing)
    const recentSteps = agentSteps.slice(-5);
    const parsedPrompt = parseSupervisorPrompt(fullContextForAI, recentSteps);
    const { valid: validMentions, invalid: invalidMentions } =
      validateAgentMentions(parsedPrompt.mentionTasks, recentSteps);

    // üîç DEBUG - Critical supervisor flow debugging
    console.log("üîç DEBUG - Clean user input:", userInput);
    console.log("üîç DEBUG - References:", references);
    console.log("üîç DEBUG - Full context for AI:", fullContextForAI);
    console.log("üîç DEBUG - Parsed mentions:", parsedPrompt.mentionTasks);
    console.log("üîç DEBUG - Valid mentions:", validMentions);
    console.log("üîç DEBUG - Invalid mentions:", invalidMentions);
    console.log("üîç DEBUG - Recent steps count:", recentSteps.length);
    console.log(
      "üîç DEBUG - All agent steps:",
      agentSteps.map((s) => ({ index: s.index, name: s.name }))
    );

    // Build supervisor context (use fullContext for AI processing)
    const supervisorHistory = supervisorTurns
      .slice(-3) // Last 3 turns for context
      .map(
        (turn) =>
          `User: ${turn.userInput}\nSupervisor: ${turn.supervisorResponse}`
      )
      .join("\n\n");

    const supervisorPrompt = buildSupervisorPrompt(
      fullContextForAI,
      recentSteps,
      supervisorHistory
    );

    // üîç DEBUG - Supervisor prompt debugging
    console.log(
      "üîç DEBUG - Supervisor prompt (first 500 chars):",
      supervisorPrompt.substring(0, 500)
    );
    console.log("üîç DEBUG - Supervisor history:", supervisorHistory);

    // Start supervisor turn in database (store clean userInput and references separately)
    const turnId = await convex.mutation(api.mutations.startSupervisorTurn, {
      sessionId: sessionId as Id<"chatSessions">,
      userInput, // Clean user input without reference context
      references: references.length > 0 ? references : undefined,
    });

    // Create streaming response
    const stream = new ReadableStream({
      async start(controller) {
        try {
          let supervisorResponse = "";
          let executedAgentUpdates: any[] = [];

          // Send turn start signal
          controller.enqueue(
            new TextEncoder().encode(
              `data: ${JSON.stringify({
                type: "supervisor_turn_start",
                turnId,
              })}\n\n`
            )
          );

          // CONVERSATION-ISOLATED FLOW: For @mentions, execute agents in conversation-only mode
          // This preserves original agent step data while enabling supervisor interactions
          if (validMentions.length > 0) {
            // üîç DEBUG - Agent execution flow
            console.log(
              "üöÄ AGENT EXECUTION START - Valid mentions found:",
              validMentions.length
            );
            console.log(
              "üöÄ AGENT EXECUTION - Mentions to process:",
              validMentions
            );

            // For @mentions, supervisor stays silent until agents complete
            // No "Processing..." message - just execute agents directly

            // Execute mentioned agents with UNIFIED streaming (eliminates duplication)
            const executedAgentUpdates: any[] = [];
            for (const mention of validMentions) {
              try {
                // Find agent by actual index, not array position
                const agentStep = agentSteps.find(
                  (step) => step.index === mention.agentIndex
                );

                if (!agentStep) {
                  console.error(
                    `‚ùå Agent step not found for index ${mention.agentIndex}`
                  );
                  continue;
                }

                console.log(
                  "‚úÖ FOUND AGENT STEP:",
                  agentStep.name,
                  agentStep.model,
                  "at index",
                  agentStep.index
                );

                // Build context from chain history
                const previousSteps = await convex.query(
                  api.queries.getPreviousAgentSteps,
                  {
                    sessionId: sessionId as Id<"chatSessions">,
                    beforeIndex: mention.agentIndex,
                  }
                );

                const chainContext = previousSteps
                  .map(
                    (step) =>
                      `${step.name || `Agent ${step.index + 1}`}: ${step.response || step.streamedContent || "No response"}`
                  )
                  .join("\n\n");

                // Get agent's conversation history
                const agentHistory = await convex.query(
                  api.queries.getAgentConversationHistory,
                  {
                    sessionId: sessionId as Id<"chatSessions">,
                    agentIndex: mention.agentIndex,
                  }
                );

                // Build the internal prompt for the agent
                const agentPrompt = buildAgentTaskPrompt(
                  mention.taskPrompt,
                  chainContext,
                  agentHistory?.conversationHistory
                    ?.map((h) => h.agentResponse)
                    .join("\n") || "",
                  mention.taskPrompt
                );

                console.log(
                  "üöÄ EXECUTING AGENT with prompt (first 200 chars):",
                  agentPrompt.substring(0, 200) + "..."
                );
                console.log("üöÄ AGENT MODEL:", agentStep.model);
                console.log("üöÄ AGENT INDEX:", mention.agentIndex);

                // CONVERSATION-ISOLATED STREAM - preserves original agent data
                let agentResponse = "";
                await streamLLM({
                  model: agentStep.model,
                  prompt: agentPrompt,
                  // NO stepId passed - prevents original agentStep data modification
                  onChunk: async (chunk: string) => {
                    agentResponse += chunk;

                    // Stream only to conversation UI (no agentStep data pollution)
                    controller.enqueue(
                      new TextEncoder().encode(
                        `data: ${JSON.stringify({
                          type: "agent_chunk",
                          agentIndex: mention.agentIndex,
                          content: chunk,
                          userPrompt: mention.taskPrompt, // Include user prompt for UI
                          turnId,
                        })}\n\n`
                      )
                    );
                  },
                  onComplete: async () => {
                    // Signal agent completion with full context
                    controller.enqueue(
                      new TextEncoder().encode(
                        `data: ${JSON.stringify({
                          type: "agent_complete",
                          agentIndex: mention.agentIndex,
                          agentName: mention.agentName,
                          response: agentResponse,
                          userPrompt: mention.taskPrompt,
                          turnId,
                        })}\n\n`
                      )
                    );

                    // Store ONLY in conversation history - DO NOT modify original agentStep
                    await convex.mutation(
                      api.mutations.appendAgentConversation,
                      {
                        sessionId: sessionId as Id<"chatSessions">,
                        agentIndex: mention.agentIndex,
                        userPrompt: mention.taskPrompt,
                        agentResponse: agentResponse,
                        triggeredBy: "supervisor",
                      }
                    );

                    executedAgentUpdates.push({
                      agentIndex: mention.agentIndex,
                      userFacingTask: mention.taskPrompt,
                      responsePreview: agentResponse.slice(0, 200),
                    });
                  },
                  onError: async (error: Error) => {
                    console.error(
                      `Agent ${mention.agentIndex} streaming error:`,
                      error
                    );
                    controller.enqueue(
                      new TextEncoder().encode(
                        `data: ${JSON.stringify({
                          type: "agent_error",
                          agentIndex: mention.agentIndex,
                          error: error.message,
                          turnId,
                        })}\n\n`
                      )
                    );
                  },
                });

                console.log(
                  "‚úÖ AGENT EXECUTION COMPLETED, response length:",
                  agentResponse.length
                );
              } catch (agentError) {
                console.error(
                  `‚ùå Agent execution failed for ${mention.agentName}:`,
                  agentError
                );
              }
            }

            // SUPERVISOR RESPONSE: Simple completion message (no streaming duplication)
            const completionMessage = `‚úÖ Routed task to ${validMentions.map((m: any) => m.agentName).join(", ")} - see results in agent columns above.`;

            controller.enqueue(
              new TextEncoder().encode(
                `data: ${JSON.stringify({
                  type: "supervisor_chunk",
                  content: completionMessage,
                  turnId,
                  isCompletion: true,
                })}\n\n`
              )
            );

            supervisorResponse = completionMessage;
          } else {
            // No mentions - normal supervisor response
            console.log(
              "‚ùå NO VALID MENTIONS - Supervisor responding directly"
            );

            // Stream supervisor response normally
            await streamLLM({
              model: "gpt-4",
              prompt: supervisorPrompt,
              onChunk: async (chunk: string) => {
                supervisorResponse += chunk;

                // Send chunk to client
                const data = JSON.stringify({
                  type: "supervisor_chunk",
                  content: chunk,
                  turnId,
                });
                controller.enqueue(
                  new TextEncoder().encode(`data: ${data}\n\n`)
                );

                // Update database with streaming content
                await convex.mutation(api.mutations.updateSupervisorTurn, {
                  turnId,
                  streamedContent: supervisorResponse,
                });
              },
              onComplete: async () => {
                // Supervisor response complete
              },
              onError: async (error: Error) => {
                throw error;
              },
            });
          }

          // Complete supervisor turn
          await convex.mutation(api.mutations.updateSupervisorTurn, {
            turnId,
            supervisorResponse,
            parsedMentions: validMentions,
            executedStepIds: [], // No new agent steps created, existing ones updated
            isComplete: true,
            isStreaming: false,
          });

          // Send completion signal
          const completeData = JSON.stringify({
            type: "supervisor_complete",
            turnId,
            executedAgents: validMentions.length,
            agentUpdates: executedAgentUpdates,
          });
          controller.enqueue(
            new TextEncoder().encode(`data: ${completeData}\n\n`)
          );

          // Handle invalid mentions if any
          if (invalidMentions.length > 0) {
            const invalidData = JSON.stringify({
              type: "invalid_mentions",
              mentions: invalidMentions,
            });
            controller.enqueue(
              new TextEncoder().encode(`data: ${invalidData}\n\n`)
            );
          }

          controller.close();
        } catch (error) {
          console.error("Supervisor interaction error:", error);

          // Update turn with error
          await convex.mutation(api.mutations.updateSupervisorTurn, {
            turnId,
            error: error instanceof Error ? error.message : "Unknown error",
            isComplete: true,
            isStreaming: false,
          });

          const errorData = JSON.stringify({
            type: "error",
            message: error instanceof Error ? error.message : "Unknown error",
          });
          controller.enqueue(
            new TextEncoder().encode(`data: ${errorData}\n\n`)
          );
          controller.close();
        }
      },
    });

    return new Response(stream, {
      headers: {
        "Content-Type": "text/stream",
        "Cache-Control": "no-cache",
        Connection: "keep-alive",
      },
    });
  } catch (error) {
    console.error("Supervisor API error:", error);
    return NextResponse.json(
      {
        error: error instanceof Error ? error.message : "Internal server error",
      },
      { status: 500 }
    );
  }
}
